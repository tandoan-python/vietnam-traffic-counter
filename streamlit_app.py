# import th∆∞ vi·ªán, s·ª≠ d·ª•ng file requirements.txt ƒë·ªÉ c√†i ƒë·∫∑t
import streamlit as st
import cv2
import tempfile
import numpy as np
import supervision as sv
from ultralytics import YOLO
from streamlit_drawable_canvas import st_canvas
from PIL import Image
import subprocess
import pandas as pd

# C·∫•u h√¨nh trang Streamlit
st.set_page_config(
    page_title="AI Traffic Analytics",
    page_icon="üöó",
    layout="wide"
)

# CSS: T√πy ch·ªânh giao di·ªán
st.markdown("""
<style>
    .main { background-color: #f8f9fa; }
    h1 { color: #d32f2f; text-align: center; }
    .stButton>button { width: 100%; background-color: #d32f2f; color: white; font-weight: bold; border-radius: 16px;}
    .stat-box { padding: 15px; background-color: white; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); text-align: center; }
    .stat-number { font-size: 2em; font-weight: bold; color: #d32f2f; }
    .tracker-info { padding: 10px; background-color: #e3f2fd; color: #0d47a1; border-radius: 5px; margin-bottom: 10px; font-size: 0.9em; }
</style>
""", unsafe_allow_html=True)

# H√†m chuy·ªÉn ƒë·ªïi video sang H.264 ƒë·ªÉ t∆∞∆°ng th√≠ch web
def convert_video_to_h264(input_path):
    output_path = input_path.replace('.mp4', '_converted.mp4')
    command = ['ffmpeg', '-y', '-i', input_path, '-vcodec', 'libx264', '-pix_fmt', 'yuv420p', output_path]
    try:
        subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return output_path
    except (subprocess.CalledProcessError, FileNotFoundError):
        return input_path

# T·∫£i model v·ªõi caching ƒë·ªÉ tr√°nh t·∫£i l·∫°i nhi·ªÅu l·∫ßn. 
# @st.cache_resource R·∫§T QUAN TR·ªåNG KH√îNG THAY ƒê·ªîI
@st.cache_resource
def load_model(model_path):
    return YOLO(model_path)

# X·ª≠ l√≠ video ƒë·∫øm ph∆∞∆°ng ti·ªán v·ªõi tracker ƒë∆∞·ª£c ch·ªçn (tr√™n giao di·ªán ch·ªçn ByteTrack ho·∫∑c BoT-SORT)
def process_video(video_path, model, zone_polygon, tracker_type):
    video_info = sv.VideoInfo.from_video_path(video_path)
    
    # X√°c ƒë·ªãnh file config cho tracker (Ultralytics c√≥ s·∫µn 2 file n√†y, ho·∫∑c c√≥ th·ªÉ t·ª± c·∫•u h√¨nh l·∫°i)
    tracker_yaml = "bytetrack.yaml" if tracker_type == "ByteTrack" else "botsort.yaml"
    
    # Kh·ªüi t·∫°o Zone: v√πng ƒë·∫øm ph∆∞∆°ng ti·ªán
    zone = sv.PolygonZone(
        polygon=zone_polygon, 
        triggering_anchors=(sv.Position.CENTER,) # R·∫§T QUAN TR·ªåNG: Ch·ªâ d√πng t√¢m h·ªôp ƒë·ªÉ tr√°nh ƒë·∫øm sai l·ªách
    )
    
    # Annotators: V·∫Ω h·ªôp, nh√£n, v√πng
    box_annotator = sv.BoxAnnotator(thickness=2)
    label_annotator = sv.LabelAnnotator(text_scale=0.5, text_thickness=1, text_padding=5)
    zone_annotator = sv.PolygonZoneAnnotator(
        zone=zone, 
        color=sv.Color.RED, 
        thickness=2, 
        text_thickness=2, 
        text_scale=1
    )

    # Video Writer ƒë·ªÉ l∆∞u video k·∫øt qu·∫£
    tfile = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
    raw_output_path = tfile.name
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(raw_output_path, fourcc, video_info.fps, video_info.resolution_wh)

    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Bi·∫øn ƒë·∫øm t√≠ch l≈©y
    class_counts = {} 
    counted_ids = set()

    # L·∫•y generator frame t·ª´ video
    frame_generator = sv.get_video_frames_generator(video_path)
    total_frames = video_info.total_frames
    
    # X·ª≠ l√Ω t·ª´ng frame
    for i, frame in enumerate(frame_generator):
        
        # S·ª≠ d·ª•ng model.track thay v√¨ model.predict: ƒë·ªÉ theo d√µi ID
        # persist=True: Nh·ªõ gi·ªØ ID gi·ªØa c√°c frame, n·∫øu ch·ªçn false s·∫Ω m·∫•t ID ngay l·∫≠p t·ª©c khi qua frame
        # tracker=tracker_yaml: Ch·ªçn thu·∫≠t to√°n (ByteTrack/BoT-SORT)
        # verbose=False: T·∫Øt log kh√¥ng c·∫ßn thi·∫øt
        results = model.track(frame, persist=True, tracker=tracker_yaml, verbose=False)[0]
        
        # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ sang Supervision
        detections = sv.Detections.from_ultralytics(results)
        
        # L∆∞u √Ω: Khi d√πng model.track, detections.tracker_id c√≥ th·ªÉ l√† None n·∫øu kh√¥ng track ƒë∆∞·ª£c
        if detections.tracker_id is not None:
            
            # Logic ƒë·∫øm (ch·ªâ ch·∫°y khi c√≥ ID)
            mask = zone.trigger(detections=detections)
            detections_in_zone = detections[mask]
            
            for tracker_id, class_id in zip(detections_in_zone.tracker_id, detections_in_zone.class_id):
                if tracker_id not in counted_ids:
                    counted_ids.add(tracker_id)
                    class_name = model.names[class_id]
                    if class_name in class_counts:
                        class_counts[class_name] += 1
                    else:
                        class_counts[class_name] = 1
        
            # V·∫Ω Label k√®m ID
            labels = [f"#{tid} {model.names[cid]}" for tid, cid in zip(detections.tracker_id, detections.class_id)]
        else:
            # N·∫øu kh√¥ng c√≥ ID (m·∫•t d·∫•u), ch·ªâ hi·ªán t√™n class
            labels = [f"{model.names[cid]}" for cid in detections.class_id]

        # V·∫Ω h√¨nh ·∫£nh k·∫øt qu·∫£
        annotated_frame = frame.copy()
        annotated_frame = zone_annotator.annotate(scene=annotated_frame)
        annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)
        annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)

        # Hi·ªÉn th·ªã th√¥ng tin Tracker v√† T·ªïng s·ªë
        cv2.putText(annotated_frame, f"Mode: {tracker_type}", (50, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
        cv2.putText(annotated_frame, f"Total IN: {len(counted_ids)}", (50, 90), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)

        out.write(annotated_frame)

        # C·∫≠p nh·∫≠t ti·∫øn tr√¨nh m·ªói 10 frame, c√≥ th·ªÉ thay ƒë·ªïi 10 th√†nh s·ªë kh√°c ƒë·ªÉ m∆∞·ª£t h∆°n
        if i % 10 == 0:
            progress_bar.progress(min(i / total_frames, 1.0))
            status_text.text(f"ƒêang x·ª≠ l√Ω frame {i}/{total_frames} | Tracker: {tracker_type} | Count: {len(counted_ids)}")

    out.release()
    progress_bar.progress(1.0)
    status_text.success("‚úÖ X·ª≠ l√Ω ho√†n t·∫•t!")
    
    final_output = convert_video_to_h264(raw_output_path)
    return final_output, class_counts

# H√†m main
def main():
    st.title("üöó H·ªá Th·ªëng Th·ªëng k√™ v√† Ph√¢n t√≠ch giao th√¥ng t·∫°i Vi·ªát Nam")

    # C·∫•u h√¨nh sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C·∫•u h√¨nh Model")
        model_source = st.radio("Ngu·ªìn Model:", ["M·∫∑c ƒë·ªãnh (yolov11m.pt)", "Ch·ªçn Yolo kh√°c (.pt)"])
        model_path = "yolov11m.pt"
        if model_source == "Ch·ªçn Yolo kh√°c (.pt)":
            up_model = st.file_uploader("File .pt", type=['pt'])
            if up_model:
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pt') as tmp:
                    tmp.write(up_model.read())
                    model_path = tmp.name

        # C·∫•u h√¨nh Tracker
        st.divider()
        st.header("üéØ C·∫•u h√¨nh Tracker")
        tracker_type = st.selectbox(
            "Ch·ªçn thu·∫≠t to√°n Tracking:",
            ("ByteTrack", "BoT-SORT"),
            help="ByteTrack: Nhanh, nh·∫π, d·ª±a tr√™n IoU.\nBoT-SORT: Ch√≠nh x√°c h∆°n khi v·∫≠t th·ªÉ b·ªã che khu·∫•t, d·ª±a tr√™n ngo·∫°i h√¨nh (ReID). \nIoU: Vi·∫øt t·∫Øt c·ªßa Intersection over Union, l√† ph√©p ƒëo trong x·ª≠ l√Ω ·∫£nh d·ª±a tr√™n Intersection (ph√©p giao) v√† Union (ph√©p h·ª£p). N√≥ l√† t√™n g·ªçi c·ªßa ch·ªâ s·ªë Jaccard trong x·ª≠ l√Ω ·∫£nh. C√¥ng th·ª©c = Di·ªán t√≠ch ph·∫ßn GIAO /Di·ªán t√≠ch ph·∫ßn H·ª¢P"
        )
        
        # Hi·ªÉn th·ªã gi·∫£i th√≠ch ng·∫Øn g·ªçn
        if tracker_type == "ByteTrack":
            st.info("**ByteTrack**: T·ªëc ƒë·ªô x·ª≠ l√Ω cao (FPS cao), ph√π h·ª£p video √≠t che khu·∫•t.")
        else:
            st.info("**BoT-SORT**: T·ªëc ƒë·ªô th·∫•p h∆°n, nh∆∞ng gi·ªØ ID t·ªët h∆°n khi xe b·ªã che ho·∫∑c chuy·ªÉn ƒë·ªông ph·ª©c t·∫°p.")

    try:
        model = load_model(model_path)
    except:
        st.error("Kh√¥ng t√¨m th·∫•y Model. H√£y t·∫£i l√™n ho·∫∑c ƒë·∫£m b·∫£o file yolov11m.pt c√≥ s·∫µn.")
        st.stop()

    uploaded_video = st.file_uploader("1. T·∫£i l√™n Video gi√°m s√°t", type=['mp4', 'avi', 'mov'])

    if uploaded_video:
        tfile = tempfile.NamedTemporaryFile(delete=False) 
        tfile.write(uploaded_video.read())
        video_path = tfile.name

        st.subheader("2. H√£y b·∫Øt ƒë·∫ßu v·∫Ω v√πng ƒë·∫øm (Entry Zone)")
        cap = cv2.VideoCapture(video_path)
        ret, frame = cap.read()
        cap.release()

        zone_polygon = None
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(frame)
            
            # T√≠nh to√°n resize (Max width 700px)
            bg_w, bg_h = pil_img.size
            canvas_w = 700
            canvas_h = int(bg_h * (canvas_w / bg_w))
            
            # Resize ·∫£nh v·ªÅ ƒë√∫ng k√≠ch th∆∞·ªõc hi·ªÉn th·ªã
            pil_img_resized = pil_img.resize((canvas_w, canvas_h))

            canvas = st_canvas(
                fill_color="rgba(255, 0, 0, 0.3)",
                stroke_width=2, stroke_color="#ff0000",
                background_image=pil_img_resized, # N·∫øu ch·∫°y local, c√≥ th·ªÉ d√πng frame tr·ª±c ti·∫øp, pil_img
                height=canvas_h, width=canvas_w,
                drawing_mode="polygon",
                key="canvas",
            )

            if canvas.json_data and canvas.json_data["objects"]:
                raw_points = canvas.json_data["objects"][-1]["path"]
                if len(raw_points) > 2:
                    
                    # Restore scale
                    scale_x = bg_w / canvas_w
                    scale_y = bg_h / canvas_h
                    
                    # # local version
                    # pts = [[int(p[1]*scale_x), int(p[2]*scale_y)] for p in raw_points if len(p)>=3]
                    
                    # Streamlit cloud version
                    pts = []
                    for p in raw_points:
                        if len(p) >= 3:
                            pts.append([int(p[1] * scale_x), int(p[2] * scale_y)])
                    

                    zone_polygon = np.array(pts)
                    st.success("‚úÖ ƒê√£ x√°c ƒë·ªãnh v√πng ƒë·∫øm!")

            if zone_polygon is not None and len(zone_polygon) >= 3:
                if st.button(f"B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH ({tracker_type})"):
                    with st.spinner(f'H·ªá th·ªëng ƒëang x·ª≠ l√≠ {tracker_type}...'):
                        
                        # Truy·ªÅn th√™m tracker_type v√†o h√†m x·ª≠ l√Ω
                        video_out, stats = process_video(video_path, model, zone_polygon, tracker_type)
                    
                    st.divider()
                    st.subheader("3. K·∫øt qu·∫£ Ph√¢n t√≠ch")
                    
                    col1, col2 = st.columns([1.5, 1])
                    
                    with col1:
                        st.video(video_out)
                    
                    with col2:
                        st.markdown(f"### üìä Th·ªëng k√™ ({tracker_type})")
                        if stats:
                            df = pd.DataFrame(list(stats.items()), columns=['Lo·∫°i', 'S·ªë l∆∞·ª£ng'])
                            total_vehicles = sum(stats.values())
                            st.markdown(f"""
                            <div class="stat-box">
                                <div>T·ªîNG PH∆Ø∆†NG TI·ªÜN ({tracker_type})</div>
                                <div class="stat-number">{total_vehicles}</div>
                            </div>
                            <br>
                            """, unsafe_allow_html=True)
                            
                            st.bar_chart(df.set_index('Lo·∫°i'), color="#d32f2f")
                            st.dataframe(df, hide_index=True, use_container_width=True)
                        else:
                            st.warning("Ch∆∞a c√≥ ph∆∞∆°ng ti·ªán n√†o ƒëi v√†o v√πng.")

if __name__ == "__main__":
    main()